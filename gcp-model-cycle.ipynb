{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cloud.google.com/python/docs/reference/aiplatform/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    # your Google Cloud Project ID or number\n",
    "    # environment default used is not set\n",
    "    project='test-project-gcp-360004',\n",
    "\n",
    "    # the Vertex AI region you will use\n",
    "    # defaults to us-central1\n",
    "    location='us-central1',\n",
    "\n",
    "    # Google Cloud Storage bucket in same region as location\n",
    "    # used to stage artifacts\n",
    "    staging_bucket='gs://my_staging_bucket',\n",
    "\n",
    "    # custom google.auth.credentials.Credentials\n",
    "    # environment default creds used if not set\n",
    "    credentials=my_credentials,\n",
    "\n",
    "    # customer managed encryption key resource name\n",
    "    # will be applied to all Vertex AI resources if set\n",
    "    encryption_spec_key_name=my_encryption_key_name,\n",
    "\n",
    "    # the name of the experiment to use to track\n",
    "    # logged metrics and parameters\n",
    "    experiment='my-experiment',\n",
    "\n",
    "    # description of the experiment above\n",
    "    experiment_description='my experiment decsription'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=\"my-dataset\", gcs_source=['gs://path/to/my/dataset.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "my_dataset = aiplatform.TextDataset.create(\n",
    "    display_name=\"my-dataset\")\n",
    "\n",
    "my_dataset.import(\n",
    "    gcs_source=['gs://path/to/my/dataset.csv']\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.multi_label_classification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aiplatform.ImageDataset('projects/my-project/location/us-central1/datasets/{DATASET_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AIP_DATA_FORMAT']  # provides format of data\n",
    "os.environ['AIP_TRAINING_DATA_URI']  # uri to training split\n",
    "os.environ['AIP_VALIDATION_DATA_URI']  # uri to validation split\n",
    "os.environ['AIP_TEST_DATA_URI']  # uri to test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AIP_MODEL_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=\"my-training-job\",\n",
    "    script_path=\"training_script.py\",\n",
    "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-cpu.2-2:latest\",\n",
    "    requirements=[\"gcsfs==0.7.1\"],\n",
    "    model_serving_container_image_uri=\"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest\",\n",
    ")\n",
    "\n",
    "model = job.run(my_dataset,\n",
    "                replica_count=1,\n",
    "                machine_type=\"n1-standard-4\",\n",
    "                accelerator_type='NVIDIA_TESLA_K80',\n",
    "                accelerator_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aiplatform.TabularDataset('projects/my-project/location/us-central1/datasets/{DATASET_ID}')\n",
    "\n",
    "job = aiplatform.AutoMLTabularTrainingJob(\n",
    "  display_name=\"train-automl\",\n",
    "  optimization_prediction_type=\"regression\",\n",
    "  optimization_objective=\"minimize-rmse\",\n",
    ")\n",
    "\n",
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    target_column=\"target_column_name\",\n",
    "    training_fraction_split=0.6,\n",
    "    validation_fraction_split=0.2,\n",
    "    test_fraction_split=0.2,\n",
    "    budget_milli_node_hours=1000,\n",
    "    model_display_name=\"my-automl-model\",\n",
    "    disable_early_stopping=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\",\n",
    "                        min_replica_count=1,\n",
    "                        max_replica_count=5\n",
    "                        machine_type='n1-standard-4',\n",
    "                        accelerator_type='NVIDIA_TESLA_K80',\n",
    "                        accelerator_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name='my-model',\n",
    "    artifact_uri=\"gs://python/to/my/model/dir\",\n",
    "    serving_container_image_uri=\"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model('/projects/my-project/locations/us-central1/models/{MODEL_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model('/projects/my-project/locations/us-central1/models/{MODEL_ID}')\n",
    "\n",
    "batch_prediction_job = model.batch_predict(\n",
    "  job_display_name='my-batch-prediction-job',\n",
    "  instances_format='csv',\n",
    "  machine_type='n1-standard-4',\n",
    "  gcs_source=['gs://path/to/my/file.csv'],\n",
    "  gcs_destination_prefix='gs://path/to/my/batch_prediction/results/',\n",
    "  service_account='my-sa@my-project.iam.gserviceaccount.com'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction_job = model.batch_predict(..., sync=False)\n",
    "\n",
    "# wait for resource to be created\n",
    "batch_prediction_job.wait_for_resource_creation()\n",
    "\n",
    "# get the state\n",
    "batch_prediction_job.state\n",
    "\n",
    "# block until job is complete\n",
    "batch_prediction_job.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.predict(instances=[[6.7, 3.1, 4.7, 1.5], [4.6, 3.1, 1.5, 0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = endpoint.create(display_name='my-endpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model('/projects/my-project/locations/us-central1/models/{MODEL_ID}')\n",
    "\n",
    "endpoint.deploy(model,\n",
    "                min_replica_count=1,\n",
    "                max_replica_count=5\n",
    "                machine_type='n1-standard-4',\n",
    "                accelerator_type='NVIDIA_TESLA_K80',\n",
    "                accelerator_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PipelineJob object\n",
    "pl = PipelineJob(\n",
    "    display_name=\"My first pipeline\",\n",
    "\n",
    "    # Whether or not to enable caching\n",
    "    # True = always cache pipeline step result\n",
    "    # False = never cache pipeline step result\n",
    "    # None = defer to cache option for each pipeline component in the pipeline definition\n",
    "    enable_caching=False,\n",
    "\n",
    "    # Local or GCS path to a compiled pipeline definition\n",
    "    template_path=\"pipeline.json\",\n",
    "\n",
    "    # Dictionary containing input parameters for your pipeline\n",
    "    parameter_values=parameter_values,\n",
    "\n",
    "    # GCS path to act as the pipeline root\n",
    "    pipeline_root=pipeline_root,\n",
    ")\n",
    "\n",
    "# Execute pipeline in Vertex AI and monitor until completion\n",
    "pl.run(\n",
    "  # Email address of service account to use for the pipeline run\n",
    "  # You must have iam.serviceAccounts.actAs permission on the service account to use it\n",
    "  service_account=service_account,\n",
    "\n",
    "  # Whether this function call should be synchronous (wait for pipeline run to finish before terminating)\n",
    "  # or asynchronous (return immediately)\n",
    "  sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PipelineJob object\n",
    "pl = PipelineJob(\n",
    "    display_name=\"My first pipeline\",\n",
    "\n",
    "    # Whether or not to enable caching\n",
    "    # True = always cache pipeline step result\n",
    "    # False = never cache pipeline step result\n",
    "    # None = defer to cache option for each pipeline component in the pipeline definition\n",
    "    enable_caching=False,\n",
    "\n",
    "    # Local or GCS path to a compiled pipeline definition\n",
    "    template_path=\"pipeline.json\",\n",
    "\n",
    "    # Dictionary containing input parameters for your pipeline\n",
    "    parameter_values=parameter_values,\n",
    "\n",
    "    # GCS path to act as the pipeline root\n",
    "    pipeline_root=pipeline_root,\n",
    ")\n",
    "\n",
    "# Submit the Pipeline to Vertex AI\n",
    "pl.submit(\n",
    "  # Email address of service account to use for the pipeline run\n",
    "  # You must have iam.serviceAccounts.actAs permission on the service account to use it\n",
    "  service_account=service_account,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform.explain.metadata.tf.v1 import saved_model_metadata_builder\n",
    "\n",
    "builder = saved_model_metadata_builder.SavedModelMetadataBuilder(\n",
    "          'gs://python/to/my/model/dir', tags=[tf.saved_model.tag_constants.SERVING]\n",
    "      )\n",
    "generated_md = builder.get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform.explain.metadata.tf.v2 import saved_model_metadata_builder\n",
    "\n",
    "builder = saved_model_metadata_builder.SavedModelMetadataBuilder('gs://python/to/my/model/dir')\n",
    "generated_md = builder.get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_metadata = builder.get_metadata_protobuf()\n",
    "\n",
    "# To deploy a model to an endpoint with explanation\n",
    "model.deploy(..., explanation_metadata=explanation_metadata)\n",
    "\n",
    "# To deploy a model to a created endpoint with explanation\n",
    "endpoint.deploy(..., explanation_metadata=explanation_metadata)\n",
    "\n",
    "# To upload a model with explanation\n",
    "aiplatform.Model.upload(..., explanation_metadata=explanation_metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
